{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc25a79-b77c-4b9e-9c0f-4bd0d2632dea",
   "metadata": {},
   "source": [
    "# Pretraining\n",
    "\n",
    "Pretrain geneformer transformer encoder on masking task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f787102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# /ihome/kyin/niandrew/.conda/envs/tf_gpu\n",
    "# /ix1/kyin/niandrew/custom_miniconda\n",
    "# source /ix1/kyin/niandrew/custom_miniconda/bin/activate tf_gpu\n",
    "!source /ix1/kyin/niandrew/custom_miniconda/bin/activate tf_gpu\n",
    "!export XLA_FLAGS=--xla_gpu_cuda_data_dir=/ihome/kyin/niandrew/.conda/envs/tf_gpu/lib/\n",
    "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/ihome/kyin/niandrew/.conda/envs/tf_gpu/lib/\n",
    "\n",
    "# !export CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\n",
    "# !export LD_LIBRARY_PATH=${CUDNN_PATH}/lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d87f3-4e22-46d5-ba92-d0a1bf02b361",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276c7ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 22:57:15.878989: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-05 22:57:15.924666: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 22:57:16.957778: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ec73cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf311a9-1a1f-4d26-af27-eb545ea86145",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52b9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing params.\n",
    "PRETRAINING_BATCH_SIZE = 12\n",
    "SEQ_LENGTH = 512\n",
    "MASK_RATE = 0.125\n",
    "PREDICTIONS_PER_SEQ = int(SEQ_LENGTH*MASK_RATE)\n",
    "\n",
    "# Model params.\n",
    "NUM_LAYERS = 6\n",
    "MODEL_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "DROPOUT = 0.02\n",
    "NORM_EPSILON = 1e-12\n",
    "\n",
    "# Training params.\n",
    "PRETRAINING_LEARNING_RATE = 1e-3\n",
    "PRETRAINING_EPOCHS = 3\n",
    "PRETRAINING_WEIGHT_DECAY = 0.001\n",
    "\n",
    "# Model name\n",
    "MODEL_NAME = 'geneformer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8108c2-2e69-4c15-87ad-f4ef387989a8",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46fbb49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25427"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Vocab File\n",
    "with open('token_dictionary.pkl', 'rb') as file:\n",
    "    vocab_dict = pickle.load(file)\n",
    "vocab_list = list(vocab_dict.keys())\n",
    "vocab_list.append('<unk>')\n",
    "\n",
    "VOCAB_SIZE = len(vocab_list)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0ef7cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 22:57:19.436602: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-05-05 22:57:19.436794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8912 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:4a:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Pretraining\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=vocab_list,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    lowercase=False,\n",
    "    strip_accents=True,\n",
    "    oov_token='<unk>'\n",
    ")\n",
    "\n",
    "# Padder\n",
    "padder = keras_nlp.layers.StartEndPacker(\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    start_value=None,\n",
    "    end_value=None,\n",
    "    pad_value=0,\n",
    "    return_padding_mask=False\n",
    ")\n",
    "\n",
    "# Masker\n",
    "masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    mask_selection_rate=MASK_RATE,\n",
    "    mask_token_id=1,\n",
    "    mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "    unselectable_token_ids=[0],\n",
    "    mask_token_rate=0.8,\n",
    "    random_token_rate=0.1\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "def preprocess(inputs):\n",
    "    outputs = padder(inputs)\n",
    "    outputs = masker(outputs)\n",
    "\n",
    "    features = {\n",
    "      \"token_ids\": outputs[\"token_ids\"],\n",
    "      \"mask_positions\": outputs[\"mask_positions\"],\n",
    "    }\n",
    "\n",
    "    labels = outputs[\"mask_ids\"]\n",
    "    weights = outputs[\"mask_weights\"]\n",
    "\n",
    "    return features, labels, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a37671fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset('ctheodoris/Genecorpus-30M', data_files='genecorpus_30M_2048.dataset/dataset.arrow', streaming=True, split='train')\n",
    "\n",
    "# Define a generator function to yield batches\n",
    "def data_generator():\n",
    "    for sample in dataset:\n",
    "        yield tf.constant(sample['input_ids'][:SEQ_LENGTH])\n",
    "\n",
    "# Create a tf.data.Dataset from the generator\n",
    "tf_dataset = tf.data.Dataset.from_generator(data_generator, tf.int32, output_shapes=[None])\n",
    "\n",
    "# Batch and pad the dataset\n",
    "tf_dataset = tf_dataset.padded_batch(\n",
    "    batch_size=PRETRAINING_BATCH_SIZE,\n",
    "    padded_shapes=SEQ_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37a23024",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'token_ids': <tf.Tensor: shape=(12, 512), dtype=int32, numpy=\n",
      "array([[11143, 17261,  5368, ..., 13399,  7733,  6285],\n",
      "       [ 1007,  8776,  7992, ..., 17712, 16565,  1364],\n",
      "       [ 8776,  1007,  7055, ...,  7027,     1,  5224],\n",
      "       ...,\n",
      "       [16725,  5368, 11143, ..., 15332,  8178,  3351],\n",
      "       [    1,  7055,  4686, ...,  2423, 14556, 12410],\n",
      "       [ 4703,  7055,  2061, ...,  2221, 13615,   220]], dtype=int32)>, 'mask_positions': <tf.Tensor: shape=(12, 64), dtype=int64, numpy=\n",
      "array([[  8,   9,  21,  23,  26,  29,  58,  59,  64,  75,  79,  82,  86,\n",
      "         88,  95,  96, 122, 128, 131, 164, 180, 181, 183, 184, 194, 202,\n",
      "        217, 229, 237, 241, 268, 288, 305, 312, 315, 316, 319, 339, 385,\n",
      "        387, 395, 398, 400, 403, 412, 413, 420, 423, 432, 433, 448, 450,\n",
      "        451, 454, 455, 461, 463, 464, 467, 469, 482, 484, 495, 499],\n",
      "       [  6,  14,  18,  23,  26,  51,  61,  70,  84,  86, 103, 107, 109,\n",
      "        118, 124, 125, 128, 137, 171, 176, 180, 190, 208, 219, 227, 231,\n",
      "        233, 234, 248, 257, 272, 274, 277, 307, 313, 316, 324, 335, 339,\n",
      "        348, 349, 366, 371, 372, 396, 399, 408, 413, 415, 420, 453, 455,\n",
      "        457, 460, 461, 463, 465, 468, 481, 482, 491, 493, 501, 508],\n",
      "       [ 22,  27,  39,  45,  47,  50,  60,  62,  63,  70,  80,  84,  88,\n",
      "        107, 114, 121, 123, 130, 134, 148, 155, 164, 173, 175, 186, 211,\n",
      "        214, 222, 225, 233, 240, 244, 248, 278, 280, 293, 295, 296, 312,\n",
      "        325, 329, 333, 342, 346, 363, 367, 368, 372, 382, 387, 391, 396,\n",
      "        420, 433, 437, 443, 444, 477, 481, 493, 502, 506, 507, 510],\n",
      "       [  3,   8,  28,  30,  38,  48,  58,  70,  81,  88,  90, 104, 107,\n",
      "        109, 143, 146, 150, 154, 157, 162, 163, 171, 176, 178, 179, 186,\n",
      "        190, 210, 213, 214, 216, 222, 230, 241, 260, 270, 280, 284, 292,\n",
      "        300, 305, 310, 315, 323, 335, 336, 348, 352, 358, 359, 378, 380,\n",
      "        409, 450, 452, 456, 458, 463, 468, 479, 481, 492, 493, 500],\n",
      "       [  6,  11,  13,  15,  29,  32,  36,  46,  54,  57,  59,  65,  71,\n",
      "         92, 111, 116, 118, 119, 140, 150, 157, 186, 192, 198, 226, 239,\n",
      "        249, 257, 262, 263, 268, 269, 286, 313, 318, 325, 326, 335, 355,\n",
      "        361, 371, 375, 396, 398, 400, 405, 407, 412, 433, 435, 438, 440,\n",
      "        445, 458, 469, 476, 479, 488, 489, 494, 497, 501, 506, 511],\n",
      "       [  3,   6,  10,  13,  36,  37,  47,  50,  62,  67,  69,  80,  90,\n",
      "         95, 102, 107, 127, 128, 131, 133, 160, 169, 184, 193, 209, 226,\n",
      "        229, 233, 248, 267, 277, 279, 292, 296, 302, 308, 309, 317, 319,\n",
      "        323, 330, 331, 334, 345, 352, 359, 361, 366, 367, 381, 382, 384,\n",
      "        389, 396, 397, 400, 407, 408, 417, 420, 446, 481, 484, 495],\n",
      "       [  5,   7,  11,  20,  21,  24,  26,  45,  48,  52,  65,  75,  77,\n",
      "         84,  95, 102, 106, 110, 114, 128, 149, 158, 160, 165, 212, 213,\n",
      "        222, 226, 241, 243, 247, 252, 279, 280, 288, 293, 316, 327, 337,\n",
      "        351, 362, 363, 368, 379, 381, 387, 388, 405, 415, 418, 420, 430,\n",
      "        440, 441, 445, 446, 448, 458, 461, 465, 490, 491, 492, 501],\n",
      "       [ 22,  23,  26,  34,  37,  42,  51,  55,  70, 107, 109, 110, 112,\n",
      "        128, 139, 144, 145, 148, 169, 172, 196, 198, 215, 220, 221, 230,\n",
      "        231, 240, 243, 258, 276, 278, 285, 287, 288, 289, 308, 313, 314,\n",
      "        316, 327, 329, 340, 355, 382, 385, 398, 407, 409, 410, 411, 414,\n",
      "        444, 454, 455, 457, 458, 462, 465, 466, 494, 497, 499, 506],\n",
      "       [ 15,  19,  24,  26,  52,  53,  54,  74,  81,  88,  98, 104, 106,\n",
      "        114, 135, 137, 138, 143, 154, 171, 174, 193, 197, 204, 214, 221,\n",
      "        238, 249, 261, 269, 273, 281, 288, 310, 312, 313, 315, 332, 336,\n",
      "        340, 370, 379, 382, 384, 386, 390, 395, 399, 400, 406, 417, 429,\n",
      "        432, 442, 445, 464, 469, 477, 478, 486, 491, 493, 497, 507],\n",
      "       [  6,  14,  26,  47,  52,  53,  66,  72,  73,  76,  80,  82,  84,\n",
      "         91, 115, 125, 135, 139, 147, 153, 171, 187, 196, 199, 203, 221,\n",
      "        237, 242, 250, 253, 259, 261, 262, 274, 275, 280, 287, 290, 293,\n",
      "        300, 313, 315, 341, 345, 367, 371, 377, 379, 387, 390, 394, 420,\n",
      "        426, 434, 436, 450, 457, 468, 473, 474, 479, 481, 499, 507],\n",
      "       [  0,  14,  17,  23,  41,  59,  62,  66,  68,  74,  85,  91,  94,\n",
      "        103, 113, 114, 118, 120, 125, 126, 134, 147, 163, 165, 175, 176,\n",
      "        196, 208, 232, 235, 240, 256, 260, 261, 270, 271, 290, 308, 320,\n",
      "        324, 334, 340, 350, 356, 378, 381, 383, 390, 396, 397, 399, 406,\n",
      "        414, 417, 427, 442, 450, 452, 464, 466, 469, 478, 483, 492],\n",
      "       [  7,   8,  23,  36,  43,  63,  67,  68,  69,  70,  73,  90,  93,\n",
      "         95,  96, 104, 107, 108, 109, 119, 122, 123, 124, 130, 132, 135,\n",
      "        136, 146, 154, 155, 168, 183, 196, 220, 230, 240, 241, 251, 256,\n",
      "        262, 266, 268, 270, 274, 282, 289, 309, 315, 322, 324, 336, 348,\n",
      "        365, 384, 395, 396, 402, 405, 406, 414, 459, 462, 502, 507]])>}, <tf.Tensor: shape=(12, 64), dtype=int32, numpy=\n",
      "array([[ 3906,  2109,  8008,  3932, 13693,  1981,  8462, 13590,  3101,\n",
      "         9561, 11938, 13620,  2100,  5910,  6423, 10863,  5779,  6221,\n",
      "        15803,  3471,  3946,  2388,  1319, 13339, 18642,  5866, 15008,\n",
      "         6031,  7027, 17075,  4553,  7078,  7657, 13744, 16708,  5037,\n",
      "         8306,  7500, 17715, 14947,  3268,  6922,  1649, 14268,  7342,\n",
      "        13062,   720,  3402, 13151, 10518,  9790,  5736, 10526,  1835,\n",
      "         4387, 24488, 12025, 17112,  1742,  7476,  5700,  8423, 11328,\n",
      "        12556],\n",
      "       [ 3777,  8446, 11293, 20426,  3790,  5716,  2377, 13788,  1374,\n",
      "         7773, 13838,  9929,  8646, 17595,  1801, 14947, 13257,  7457,\n",
      "        12556, 17314, 18666,  6071,  1626,  8398, 20453,  3081,  4737,\n",
      "        11897, 12086,   435,  3996,  4441, 19823, 12790, 13272,   608,\n",
      "        12172, 14157, 10511,  4715,  7183, 10526,  2195,  1965,  2453,\n",
      "        11695,  7593,  6867, 12068,  1850,  1773,  3840,  2961,  5312,\n",
      "        17715,  4187,  8231,  2368,  9388, 11401,  3475,  8917, 14628,\n",
      "        12324],\n",
      "       [ 8052,  5538,  3777, 10262,   664,  7733, 17906,  9347,  1666,\n",
      "         3528, 11937,  3906,  6853,  4703,  4466, 16370,  4547,  6582,\n",
      "         5826,  4561,   516, 13749,  4251,  3971,  2453, 16913, 15415,\n",
      "         1833,  6419, 13151,  8135, 11576, 20384, 22881,  4275,  7992,\n",
      "         9536, 15993,   908,  7773,  4409, 17715,  3020, 18373,  9563,\n",
      "         9138,  1631,  5704,    60,  9154,  9358, 11828,  2800,  3502,\n",
      "         3236, 11123,  8674, 10639,  8642,  4350,  5791, 13594,   787,\n",
      "        13216],\n",
      "       [  454,  1311,  1729,  1456,  9945, 19475,  9051,  7817,  8416,\n",
      "        13620, 15635,  5045, 11496, 12025,  3502,  8205,  4273, 13900,\n",
      "        13267,  7502, 16536,  3351,  5891, 17628, 13440, 16980, 17280,\n",
      "         4857,  4737,  9367, 14983, 12313,  5613,  3942, 10000, 10637,\n",
      "        17261,  8883,  9536,  9180,  2289, 12244, 16734, 13193, 10611,\n",
      "        24595, 10639,  7209,  2928,  5776,  7302,  1435,   874, 17564,\n",
      "        13259,  6840, 15603,  3267, 24170, 10136,  3594,  9004, 13227,\n",
      "        11670],\n",
      "       [ 7411, 12744,    76, 11727,  5538,  6533, 16239, 10604,  2985,\n",
      "         1823, 13620, 13257,  1318,  1806, 19884, 14956,  7345,  3942,\n",
      "        15425,  9718, 16366, 16682,  4441,  7962,  3792,  3042, 10352,\n",
      "        12823, 17314,  8205,  8446,  1147,  1626,  9347, 14498,  4485,\n",
      "         4170, 21042, 11572, 16942,  5182,  8471,  2016,  7391, 13272,\n",
      "         4532,  8565, 16212,  2758,  5716,   294, 12983, 20858, 16165,\n",
      "         5332,  4715,  2221,  8666,  3292, 19475,  2870,  5906,  2786,\n",
      "         5891],\n",
      "       [ 7055,  8323,  3906,  4686,  2359, 11938, 11123,  8953,  2382,\n",
      "         1007,  2195,  5557,  5823, 16239,  1666, 12245, 15803,  9280,\n",
      "         1456,  4821,  8917,  1806,  8311, 15596, 11404, 16737,  3664,\n",
      "         2377,  9627, 15152,  5332,  4205,  2816,  1984,  4798,  5030,\n",
      "        15510, 13327,  2146,  9646, 10604,   253, 19901,  2743, 15251,\n",
      "         7203,   776,  5107,  4857,  5261,   856, 11897,   169,  7962,\n",
      "         3037,  7078,  7252,  5666,  9820,  1147, 12409,  6247,  2095,\n",
      "        16237],\n",
      "       [12039, 15243,  7055,  7903,  6907, 11245,  5750, 17261, 17280,\n",
      "         6533,  1626,  7250, 12015,  6221, 13486,  6861,  1720, 13732,\n",
      "         7326, 12428, 18526,  6038,  4257,  7422,  1965,  9994,  6251,\n",
      "         9816,  1336,  7565,  7335, 13506,  2336,  9936,  1806,   352,\n",
      "        11572, 10390,  7330,  2928, 10863,  4547, 13455,  6070,  3792,\n",
      "        12530,  9581,  5785,  5531, 14180, 10943,  3621,  6561,  9497,\n",
      "        10526,  6466,  4466,  9254, 14125,  2198, 13403,  6019,  5760,\n",
      "         3454],\n",
      "       [ 8907,  2786,  8008,  4189, 11861,  3932,  5813, 19925,  2109,\n",
      "         9870,  3664,  2161, 20356, 15452,  3991,  2657,  2870,  4350,\n",
      "        12686,  2216,  1987, 12324,  5533, 13590, 14956,  3092,  7500,\n",
      "         9025, 10819, 12636,  3857,   549, 13698,  4496,  3522,  6285,\n",
      "         3827,  3292,  4339, 16079, 11498, 12561,  9561,  1835,  6120,\n",
      "         5850,  2196,  9076,  4609, 17015,  7295,  8446,     2,    76,\n",
      "         5718,  8898, 11212,  5746,  3607,  2527, 12723,  9811,  9452,\n",
      "        13801],\n",
      "       [  608,  8454,  1101,  5785,  4127, 17911,  8666, 11712,  2432,\n",
      "         3236, 12971,  5666,  2669, 15056,  2109,  9340,  1513,  5538,\n",
      "         3351,  2775,  5609,  5266, 19925,  5246,  1823, 15091,   253,\n",
      "         4287,   704, 14280,  6450,  7348, 23846,  4867, 11177,  6091,\n",
      "        12884, 11799, 10465, 24121, 10927, 14116, 16913, 13732, 17610,\n",
      "         3758, 16701, 12723,  4562,  7171, 13574, 20340, 11172,  1870,\n",
      "         1275,  5180, 13103, 13112, 20356,  7029,  4890, 18639,  9151,\n",
      "         2272],\n",
      "       [ 5794, 15481,  2874,  1279, 12814,  4339,  2310,  2646,  5760,\n",
      "        16239, 14903,  8160,  7962,  2364, 15190, 10287,   776,  1513,\n",
      "        15699,  1803,  3795,  2130, 12218, 15597, 13098,  4541, 12652,\n",
      "         4358, 13440,  8135,  2382, 14646,  4167,  6391,  6417,  9461,\n",
      "         3575,  1695,  8416, 10601,  8967,  1815, 12823, 15880, 13083,\n",
      "        11875,  4547,  1470,  2295, 11090,  4860, 15003,  4214, 16701,\n",
      "        21560,  7618,  8355, 13569,  1474,   964,  4565,  5677, 19901,\n",
      "        11858],\n",
      "       [ 5557, 15243,  2061,  6840, 10837, 12218,  4980, 20023,   688,\n",
      "        12324, 13257,   576,  1631, 11032,  2725,  2359, 17269, 12556,\n",
      "         3298, 11785,  9911, 12378,  2198,  7379, 18591,  6423,  2657,\n",
      "        14367,  8264,  8357,  4275, 15803,  2195,  9025,  6549,   859,\n",
      "         2749,  3141,  9091,  4280,  6206, 16682, 15488,  7521,  3792,\n",
      "        13399, 19901, 16554, 12164,  6717, 11536,  4839, 10511,  6746,\n",
      "        10189,  1051,  8522,  5463, 17297,  1394, 15993, 21343,  5541,\n",
      "        12419],\n",
      "       [ 2336,  8323,  3471,  3932,  3717,  6545,  1137,  8165, 12790,\n",
      "         9994, 16906, 13447,  7203,  4088,   776, 20334,  3827,  5368,\n",
      "         4839,  2364,  8018, 10518, 15289,  5826, 13173,  6104,  4821,\n",
      "         9220, 11576, 10467,  5777,  9347,  4880, 16370, 17581,  3739,\n",
      "        16048,   362,  8462, 14983,  8446,   435,  4433, 14844,  1614,\n",
      "         9575,  7954,  4466,  6427, 11856,  4387,  8285,  3402,  4690,\n",
      "        17280, 17295,  9131, 15255,  9936, 16165, 14956,  4037, 11406,\n",
      "         7238]], dtype=int32)>, <tf.Tensor: shape=(12, 64), dtype=float32, numpy=\n",
      "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
      "      dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute preprocessed batches on the fly on the CPU.\n",
    "pretrain_ds = tf_dataset.map(\n",
    "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(pretrain_ds.take(1).get_single_element())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28162c59-495e-4787-a5fe-bf7dfe0f95e6",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078158d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">6,640,384</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m6,640,384\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ layer_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │       \u001b[38;5;34m527,104\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,803,520</span> (37.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,803,520\u001b[0m (37.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,803,520</span> (37.40 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,803,520\u001b[0m (37.40 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\")\n",
    "\n",
    "# Embed our tokens with a positional embedding.\n",
    "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=VOCAB_SIZE,\n",
    "    sequence_length=SEQ_LENGTH,\n",
    "    embedding_dim=MODEL_DIM,\n",
    "    mask_zero=True\n",
    ")\n",
    "outputs = embedding_layer(inputs)\n",
    "\n",
    "# Apply layer normalization and dropout to the embedding.\n",
    "outputs = keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "outputs = keras.layers.Dropout(rate=DROPOUT)(outputs)\n",
    "\n",
    "# Add encoder blocks\n",
    "for i in range(NUM_LAYERS):\n",
    "    outputs = keras_nlp.layers.TransformerEncoder(\n",
    "        intermediate_dim=INTERMEDIATE_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT,\n",
    "        layer_norm_epsilon=NORM_EPSILON,\n",
    "    )(outputs)\n",
    "\n",
    "encoder_model = keras.Model(inputs, outputs)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e733b375-4343-43e8-9cd4-0a7b3db3b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "# tensorboard --host 0.0.0.0 --logdir logs/fit --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecc0aea-730b-42bf-966f-5962d093d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4de9c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)      </span>┃<span style=\"font-weight: bold\"> Output Shape    </span>┃<span style=\"font-weight: bold\">   Param # </span>┃<span style=\"font-weight: bold\"> Connected to   </span>┃<span style=\"font-weight: bold\"> Trai… </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ token_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -              │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ functional_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">9,803,520</span> │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └ input_layer  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -              │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6,640,384</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ token_and_positi… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositio…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ layer_normalizat… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalizat…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └ dropout      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -              │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>,     │   <span style=\"color: #00af00; text-decoration-color: #00af00\">527,104</span> │ -              │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ transformer_enco… │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │           │                │       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEnco…</span> │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ mask_positions    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -              │   <span style=\"font-weight: bold\">-</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ masked_lm_head    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">6,601,043</span> │ functional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │   <span style=\"color: #00af00; text-decoration-color: #00af00; font-weight: bold\">Y</span>   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaskedLMHead</span>)    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">25427</span>)          │           │ mask_position… │       │\n",
       "└───────────────────┴─────────────────┴───────────┴────────────────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m  Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTrai…\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━┩\n",
       "│ token_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m0\u001b[0m │ -              │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ functional_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │ \u001b[38;5;34m9,803,520\u001b[0m │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)      │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └ input_layer  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │         \u001b[38;5;34m0\u001b[0m │ -              │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │ \u001b[38;5;34m6,640,384\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ token_and_positi… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTokenAndPositio…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │       \u001b[38;5;34m512\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ layer_normalizat… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mLayerNormalizat…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └ dropout      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │         \u001b[38;5;34m0\u001b[0m │ -              │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)         │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│    └              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m,     │   \u001b[38;5;34m527,104\u001b[0m │ -              │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ transformer_enco… │ \u001b[38;5;34m256\u001b[0m)            │           │                │       │\n",
       "│ (\u001b[38;5;33mTransformerEnco…\u001b[0m │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ mask_positions    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │         \u001b[38;5;34m0\u001b[0m │ -              │   \u001b[1m-\u001b[0m   │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)      │                 │           │                │       │\n",
       "├───────────────────┼─────────────────┼───────────┼────────────────┼───────┤\n",
       "│ masked_lm_head    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m,      │ \u001b[38;5;34m6,601,043\u001b[0m │ functional_1[\u001b[38;5;34m…\u001b[0m │   \u001b[1;38;5;34mY\u001b[0m   │\n",
       "│ (\u001b[38;5;33mMaskedLMHead\u001b[0m)    │ \u001b[38;5;34m25427\u001b[0m)          │           │ mask_position… │       │\n",
       "└───────────────────┴─────────────────┴───────────┴────────────────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,895,251</span> (37.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,895,251\u001b[0m (37.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,895,251</span> (37.75 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,895,251\u001b[0m (37.75 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the pretraining model by attaching a masked language model head.\n",
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(SEQ_LENGTH,), dtype=\"int32\", name=\"token_ids\"),\n",
    "    \"mask_positions\": keras.Input(\n",
    "        shape=(PREDICTIONS_PER_SEQ,), dtype=\"int32\", name=\"mask_positions\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Encode the tokens.\n",
    "encoded_tokens = encoder_model(inputs[\"token_ids\"])\n",
    "\n",
    "# Predict an output word for each masked input token.\n",
    "# We use the input token embedding to project from our encoded vectors to\n",
    "# vocabulary logits, which has been shown to improve training efficiency.\n",
    "outputs = keras_nlp.layers.MaskedLMHead(\n",
    "    token_embedding=embedding_layer.token_embedding,\n",
    "    activation=\"softmax\",\n",
    ")(encoded_tokens, mask_positions=inputs[\"mask_positions\"])\n",
    "\n",
    "# Define and compile our pretraining model.\n",
    "pretraining_model = keras.Model(inputs, outputs)\n",
    "pretraining_model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=PRETRAINING_LEARNING_RATE,\n",
    "                                        weight_decay=PRETRAINING_WEIGHT_DECAY),\n",
    "    weighted_metrics=[\"sparse_categorical_accuracy\"],\n",
    "    jit_compile=True\n",
    ")\n",
    "\n",
    "pretraining_model.summary(expand_nested=True, show_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2586741-0a97-4e04-802e-2351b74f8551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  12849/Unknown \u001b[1m279s\u001b[0m 21ms/step - loss: 5.1817 - sparse_categorical_accuracy: 0.0025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pretrain the model on our wiki text dataset.\n",
    "history = pretraining_model.fit(\n",
    "    pretrain_ds,\n",
    "    epochs=PRETRAINING_EPOCHS,\n",
    "    shuffle=True, \n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea104159-66e4-4877-8abf-0f7f85d6da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save this base model for further finetuning\n",
    "encoder_model.save((\"model/\" + MODEL_NAME + \".keras\"))\n",
    "\n",
    "# Save training history\n",
    "with open(\"model/\" + MODEL_NAME + \"_history.pkl\", 'wb') as file:\n",
    "    pickle.dump(history, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
